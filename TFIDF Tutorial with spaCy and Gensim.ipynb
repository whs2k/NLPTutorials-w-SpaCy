{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn on Logging\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2017-09-08 09:17:16,503 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/whs/Documents/Fun With ML/Gensim Tutorials'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in 'documents'\n",
    "\n",
    "raw1 = 'The cute kitten purred and watched the Stark girl.'\n",
    "raw2 = ' After losing interest the cute furry cat purred and meowed.'\n",
    "raw3 = ' Sly yet silent, the cute kitten meowed and she noticed.' \n",
    "raw4 = ' The loud furry dog ran and bit at air.'\n",
    "\n",
    "raw_text = str(raw1 + raw2 + raw3 + raw4)\n",
    "raw_text\n",
    "raw_text2 = str(raw1 + raw4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process text w/ SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize 'en' tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push all documents through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The cute kitten purred and watched the Stark girl. After losing interest the cute furry cat purred and meowed. Sly yet silent, the cute kitten meowed and she noticed. The loud furry dog ran and bit at air."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = nlp(raw_text, parse=True)\n",
    "docs2 = nlp(raw_text2, parse=True)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Text Analysis Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "The cute kitten purred and watched the Stark girl.\n",
      "After losing interest the cute furry cat purred and meowed.\n",
      "Sly yet silent, the cute kitten meowed and she noticed.\n",
      "The loud furry dog ran and bit at air.\n"
     ]
    }
   ],
   "source": [
    "#Now with our text all spacy tokened, we can do cool stuff\n",
    "#https://github.com/cytora/pycon-nlp-in-10-lines\n",
    "\n",
    "# Get first token of the processed document\n",
    "token = docs[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in docs.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - DET\n",
      "cute - ADJ\n",
      "kitten - NOUN\n",
      "purred - VERB\n",
      "and - CCONJ\n",
      "watched - VERB\n",
      "the - DET\n",
      "Stark - PROPN\n",
      "girl - NOUN\n",
      ". - PUNCT\n",
      "After - ADP\n",
      "losing - VERB\n",
      "interest - NOUN\n",
      "the - DET\n",
      "cute - ADJ\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in docs[0:15]:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stark - PERSON\n",
      "Sly - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "for ent in docs.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821555381691\n",
      "0.801685591074\n",
      "0.703533825482\n",
      "0.280267313562\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'cat', 'kitten', and 'dog'\n",
    "kitten = docs[2]\n",
    "cat = docs[16]\n",
    "dog = docs[36]\n",
    "yet = docs[22]\n",
    "print(cat.similarity(kitten))\n",
    "print(cat.similarity(dog))\n",
    "print(kitten.similarity(dog))\n",
    "print(yet.similarity(dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus and Dictionary w/ Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4 sentences found. Here's a sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The cute kitten purred and watched the Stark girl.',\n",
       " 'After losing interest the cute furry cat purred and meowed.',\n",
       " 'Sly yet silent, the cute kitten meowed and she noticed.',\n",
       " 'The loud furry dog ran and bit at air.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence.orth_ for sentence in docs.sents]\n",
    "print(\"There were {} sentences found. Here's a sample:\".format(len(sentences)))\n",
    "pd.DataFrame(sentences[0:5])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Each Word by Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute']\n",
      "['cute', 'kitten']\n",
      "['cute', 'kitten', 'purr']\n",
      "['cute', 'kitten', 'purr', 'watch']\n",
      "['cute', 'kitten', 'purr', 'watch', 'stark']\n",
      "['cute', 'kitten', 'purr', 'watch', 'stark', 'girl']\n",
      "['lose']\n",
      "['lose', 'interest']\n",
      "['lose', 'interest', 'cute']\n",
      "['lose', 'interest', 'cute', 'furry']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat', 'purr']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat', 'purr', 'meow']\n",
      "['sly']\n",
      "['sly', 'silent']\n",
      "['sly', 'silent', 'cute']\n",
      "['sly', 'silent', 'cute', 'kitten']\n",
      "['sly', 'silent', 'cute', 'kitten', 'meow']\n",
      "['sly', 'silent', 'cute', 'kitten', 'meow', 'notice']\n",
      "['loud']\n",
      "['loud', 'furry']\n",
      "['loud', 'furry', 'dog']\n",
      "['loud', 'furry', 'dog', 'run']\n",
      "['loud', 'furry', 'dog', 'run', 'bit']\n",
      "['loud', 'furry', 'dog', 'run', 'bit', 'air']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Each Word\n",
    "texts, article = [], []\n",
    "for w in docs:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "        print(article)\n",
    "        #texts.append(article)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.tag_ == '.' :\n",
    "        texts.append(article)\n",
    "        #print(article) #Print out each word to test\n",
    "        article = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cute', 'kitten', 'purr', 'watch', 'stark', 'girl'],\n",
       " ['lose', 'interest', 'cute', 'furry', 'cat', 'purr', 'meow'],\n",
       " ['sly', 'silent', 'cute', 'kitten', 'meow', 'notice'],\n",
       " ['loud', 'furry', 'dog', 'run', 'bit', 'air']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine all lines into one list\n",
    "#for sentances in document for sentences \n",
    "texts_single = [item for sublist in texts for item in sublist]\n",
    "\n",
    "#This list comprehension is doing the same as:\n",
    "#for sublist in texts:\n",
    "#    for item in sublist:\n",
    "#        flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute', 'kitten', 'purr', 'watch', 'stark']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_single[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-08 09:17:21,761 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-09-08 09:17:21,763 : INFO : built Dictionary(19 unique tokens: ['cute', 'kitten', 'purr', 'watch', 'stark']...) from 4 documents (total 25 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cute': 0, 'kitten': 1, 'purr': 2, 'watch': 3, 'stark': 4, 'girl': 5, 'lose': 6, 'interest': 7, 'furry': 8, 'cat': 9, 'meow': 10, 'sly': 11, 'silent': 12, 'notice': 13, 'loud': 14, 'dog': 15, 'run': 16, 'bit': 17, 'air': 18}\n",
      "Dictionary(19 unique tokens: ['cute', 'kitten', 'purr', 'watch', 'stark']...)\n"
     ]
    }
   ],
   "source": [
    "#Make Dictionary \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#dictionary.save(('dovel1.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary.token2id)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Matrix Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#corpora.MmCorpus.serialize('dovel1.mm', corpus)  # store to disk, for later use\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-08 09:17:21,788 : INFO : collecting document frequencies\n",
      "2017-09-08 09:17:21,790 : INFO : PROGRESS: processing document #0\n",
      "2017-09-08 09:17:21,793 : INFO : calculating IDF weights for 4 documents and 18 features (25 matrix non-zeros)\n",
      "2017-09-08 09:17:21,794 : INFO : collecting document frequencies\n",
      "2017-09-08 09:17:21,796 : INFO : PROGRESS: processing document #0\n",
      "2017-09-08 09:17:21,797 : INFO : calculating IDF weights for 4 documents and 18 features (25 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=25)\n"
     ]
    }
   ],
   "source": [
    "#initilize\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidfN = models.TfidfModel(corpus, normalize=False)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = corpus[0]\n",
    "s2 = corpus[1]\n",
    "s3 = corpus[2]\n",
    "s4 = corpus[3]\n",
    "\n",
    "tf1=tfidf[s1]\n",
    "tf2=tfidf[s2]\n",
    "tf3=tfidf[s3]\n",
    "tf4=tfidf[s4]\n",
    "\n",
    "tf1N=tfidfN[s1]\n",
    "tf2N=tfidfN[s2]\n",
    "tf3N=tfidfN[s3]\n",
    "tf4N=tfidfN[s4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.11024726933725056), (1, 0.2656320682560318), (2, 0.2656320682560318), (3, 0.5312641365120636), (4, 0.5312641365120636), (5, 0.5312641365120636)]\n",
      "[(0, 0.10655215922886847), (2, 0.25672899295608276), (6, 0.5134579859121655), (7, 0.5134579859121655), (8, 0.25672899295608276), (9, 0.5134579859121655), (10, 0.25672899295608276)]\n",
      "[(0, 0.4150374992788437), (1, 1.0), (2, 1.0), (3, 2.0), (4, 2.0), (5, 2.0)]\n",
      "[(0, 0.4150374992788437), (2, 1.0), (6, 2.0), (7, 2.0), (8, 1.0), (9, 2.0), (10, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(tf1)\n",
    "print(tf2)\n",
    "print(tf1N)\n",
    "print(tf2N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['Sentence'] = [raw1, raw2, raw3, raw4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tokenization Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['Tokens'] = [texts[0], texts[1], texts[2],texts[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Corpus Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['Corpus'] = [s1, s2, s3, s4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['TFIDF'] = [tf1, tf2, tf3, tf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['TFIDF Not Normalized'] = [tf1N, tf2N, tf3N, tf4N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Corpus</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>TFIDF Not Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cute kitten purred and watched the Stark g...</td>\n",
       "      <td>[cute, kitten, purr, watch, stark, girl]</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]</td>\n",
       "      <td>[(0, 0.11024726933725056), (1, 0.2656320682560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (1, 1.0), (2, 1.0), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After losing interest the cute furry cat purr...</td>\n",
       "      <td>[lose, interest, cute, furry, cat, purr, meow]</td>\n",
       "      <td>[(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1...</td>\n",
       "      <td>[(0, 0.10655215922886847), (2, 0.2567289929560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (2, 1.0), (6, 2.0), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sly yet silent, the cute kitten meowed and sh...</td>\n",
       "      <td>[sly, silent, cute, kitten, meow, notice]</td>\n",
       "      <td>[(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (1...</td>\n",
       "      <td>[(0, 0.11024726933725056), (1, 0.2656320682560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (1, 1.0), (10, 1.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The loud furry dog ran and bit at air.</td>\n",
       "      <td>[loud, furry, dog, run, bit, air]</td>\n",
       "      <td>[(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (...</td>\n",
       "      <td>[(8, 0.2182178902359924), (14, 0.4364357804719...</td>\n",
       "      <td>[(8, 1.0), (14, 2.0), (15, 2.0), (16, 2.0), (1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  The cute kitten purred and watched the Stark g...   \n",
       "1   After losing interest the cute furry cat purr...   \n",
       "2   Sly yet silent, the cute kitten meowed and sh...   \n",
       "3             The loud furry dog ran and bit at air.   \n",
       "\n",
       "                                           Tokens  \\\n",
       "0        [cute, kitten, purr, watch, stark, girl]   \n",
       "1  [lose, interest, cute, furry, cat, purr, meow]   \n",
       "2       [sly, silent, cute, kitten, meow, notice]   \n",
       "3               [loud, furry, dog, run, bit, air]   \n",
       "\n",
       "                                              Corpus  \\\n",
       "0   [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]   \n",
       "1  [(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1...   \n",
       "2  [(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (1...   \n",
       "3  [(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [(0, 0.11024726933725056), (1, 0.2656320682560...   \n",
       "1  [(0, 0.10655215922886847), (2, 0.2567289929560...   \n",
       "2  [(0, 0.11024726933725056), (1, 0.2656320682560...   \n",
       "3  [(8, 0.2182178902359924), (14, 0.4364357804719...   \n",
       "\n",
       "                                TFIDF Not Normalized  \n",
       "0  [(0, 0.4150374992788437), (1, 1.0), (2, 1.0), ...  \n",
       "1  [(0, 0.4150374992788437), (2, 1.0), (6, 2.0), ...  \n",
       "2  [(0, 0.4150374992788437), (1, 1.0), (10, 1.0),...  \n",
       "3  [(8, 1.0), (14, 2.0), (15, 2.0), (16, 2.0), (1...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-27-daea73d812e5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-daea73d812e5>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "#From https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "    #weight_{i,j} = frequency_{i,j} * log_2(D / document_freq_{i})\n",
    "    = \n",
    "\n",
    "# 'loud' in row 3 TF = 1; D = 4; DF = 1  -> TF/DF = 1 * log_2(4/1)\n",
    "# 'cute' in row 0 sentance TF = 1 & DF = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
