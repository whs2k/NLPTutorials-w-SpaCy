{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn on Logging\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/whs/Documents/Fun With ML/Gensim Tutorials'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in 'documents'\n",
    "\n",
    "raw1 = 'The cute kitten purred and watched the Stark girl.'\n",
    "raw2 = ' After losing interest the cute furry cat purred and meowed.'\n",
    "raw3 = ' Sly yet silent, the cute kitten meowed and she noticed.' \n",
    "raw4 = ' The loud furry dog ran and bit at air.'\n",
    "\n",
    "raw_text = str(raw1 + raw2 + raw3 + raw4)\n",
    "raw_text\n",
    "raw_text2 = str(raw1 + raw4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process text w/ SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize 'en' tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push all documents through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The cute kitten purred and watched the Stark girl. After losing interest the cute furry cat purred and meowed. Sly yet silent, the cute kitten meowed and she noticed. The loud furry dog ran and bit at air."
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = nlp(raw_text, parse=True)\n",
    "docs2 = nlp(raw_text2, parse=True)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Text Analysis Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "The cute kitten purred and watched the Stark girl.\n",
      "After losing interest the cute furry cat purred and meowed.\n",
      "Sly yet silent, the cute kitten meowed and she noticed.\n",
      "The loud furry dog ran and bit at air.\n"
     ]
    }
   ],
   "source": [
    "#Now with our text all spacy tokened, we can do cool stuff\n",
    "#https://github.com/cytora/pycon-nlp-in-10-lines\n",
    "\n",
    "# Get first token of the processed document\n",
    "token = docs[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in docs.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - DET\n",
      "cute - ADJ\n",
      "kitten - NOUN\n",
      "purred - VERB\n",
      "and - CCONJ\n",
      "watched - VERB\n",
      "the - DET\n",
      "Stark - PROPN\n",
      "girl - NOUN\n",
      ". - PUNCT\n",
      "After - ADP\n",
      "losing - VERB\n",
      "interest - NOUN\n",
      "the - DET\n",
      "cute - ADJ\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in docs[0:15]:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stark - PERSON\n",
      "Sly - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "for ent in docs.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821555381691\n",
      "0.801685591074\n",
      "0.703533825482\n",
      "0.280267313562\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'cat', 'kitten', and 'dog'\n",
    "kitten = docs[2]\n",
    "cat = docs[16]\n",
    "dog = docs[36]\n",
    "yet = docs[22]\n",
    "print(cat.similarity(kitten))\n",
    "print(cat.similarity(dog))\n",
    "print(kitten.similarity(dog))\n",
    "print(yet.similarity(dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus and Dictionary w/ Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4 sentences found. Here's a sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The cute kitten purred and watched the Stark girl.',\n",
       " 'After losing interest the cute furry cat purred and meowed.',\n",
       " 'Sly yet silent, the cute kitten meowed and she noticed.',\n",
       " 'The loud furry dog ran and bit at air.']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence.orth_ for sentence in docs.sents]\n",
    "print(\"There were {} sentences found. Here's a sample:\".format(len(sentences)))\n",
    "pd.DataFrame(sentences[0:5])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Each Word by Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute']\n",
      "['cute', 'kitten']\n",
      "['cute', 'kitten', 'purr']\n",
      "['cute', 'kitten', 'purr', 'watch']\n",
      "['cute', 'kitten', 'purr', 'watch', 'stark']\n",
      "['cute', 'kitten', 'purr', 'watch', 'stark', 'girl']\n",
      "['lose']\n",
      "['lose', 'interest']\n",
      "['lose', 'interest', 'cute']\n",
      "['lose', 'interest', 'cute', 'furry']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat', 'purr']\n",
      "['lose', 'interest', 'cute', 'furry', 'cat', 'purr', 'meow']\n",
      "['sly']\n",
      "['sly', 'silent']\n",
      "['sly', 'silent', 'cute']\n",
      "['sly', 'silent', 'cute', 'kitten']\n",
      "['sly', 'silent', 'cute', 'kitten', 'meow']\n",
      "['sly', 'silent', 'cute', 'kitten', 'meow', 'notice']\n",
      "['loud']\n",
      "['loud', 'furry']\n",
      "['loud', 'furry', 'dog']\n",
      "['loud', 'furry', 'dog', 'run']\n",
      "['loud', 'furry', 'dog', 'run', 'bit']\n",
      "['loud', 'furry', 'dog', 'run', 'bit', 'air']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Each Word\n",
    "texts, article = [], []\n",
    "for w in docs:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "        print(article)\n",
    "        #texts.append(article)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.tag_ == '.' :\n",
    "        texts.append(article)\n",
    "        #print(article) #Print out each word to test\n",
    "        article = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cute', 'kitten', 'purr', 'watch', 'stark', 'girl'],\n",
       " ['lose', 'interest', 'cute', 'furry', 'cat', 'purr', 'meow'],\n",
       " ['sly', 'silent', 'cute', 'kitten', 'meow', 'notice'],\n",
       " ['loud', 'furry', 'dog', 'run', 'bit', 'air']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine all lines into one list\n",
    "#for sentances in document for sentences \n",
    "texts_single = [item for sublist in texts for item in sublist]\n",
    "\n",
    "#This list comprehension is doing the same as:\n",
    "#for sublist in texts:\n",
    "#    for item in sublist:\n",
    "#        flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute', 'kitten', 'purr', 'watch', 'stark']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_single[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cute': 0, 'kitten': 1, 'purr': 2, 'watch': 3, 'stark': 4, 'girl': 5, 'lose': 6, 'interest': 7, 'furry': 8, 'cat': 9, 'meow': 10, 'sly': 11, 'silent': 12, 'notice': 13, 'loud': 14, 'dog': 15, 'run': 16, 'bit': 17, 'air': 18}\n",
      "Dictionary(19 unique tokens: ['cute', 'kitten', 'purr', 'watch', 'stark']...)\n"
     ]
    }
   ],
   "source": [
    "#Make Dictionary \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#dictionary.save(('dovel1.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary.token2id)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Matrix Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#corpora.MmCorpus.serialize('dovel1.mm', corpus)  # store to disk, for later use\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=25)\n"
     ]
    }
   ],
   "source": [
    "#initilize\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidfN = models.TfidfModel(corpus, normalize=False)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = corpus[0]\n",
    "s2 = corpus[1]\n",
    "s3 = corpus[2]\n",
    "s4 = corpus[3]\n",
    "\n",
    "tf1=tfidf[s1]\n",
    "tf2=tfidf[s2]\n",
    "tf3=tfidf[s3]\n",
    "tf4=tfidf[s4]\n",
    "\n",
    "tf1N=tfidfN[s1]\n",
    "tf2N=tfidfN[s2]\n",
    "tf3N=tfidfN[s3]\n",
    "tf4N=tfidfN[s4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.11024726933725056), (1, 0.2656320682560318), (2, 0.2656320682560318), (3, 0.5312641365120636), (4, 0.5312641365120636), (5, 0.5312641365120636)]\n",
      "[(0, 0.10655215922886847), (2, 0.25672899295608276), (6, 0.5134579859121655), (7, 0.5134579859121655), (8, 0.25672899295608276), (9, 0.5134579859121655), (10, 0.25672899295608276)]\n",
      "[(0, 0.4150374992788437), (1, 1.0), (2, 1.0), (3, 2.0), (4, 2.0), (5, 2.0)]\n",
      "[(0, 0.4150374992788437), (2, 1.0), (6, 2.0), (7, 2.0), (8, 1.0), (9, 2.0), (10, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(tf1)\n",
    "print(tf2)\n",
    "print(tf1N)\n",
    "print(tf2N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Token:  cute\n",
      "Vector Size:  300\n",
      "[-0.35642001 -0.12153    -0.60569    -0.062242   -0.12673    -0.02612\n",
      " -0.058334   -0.59749001  0.080791    1.00230002 -0.30669999 -0.49897999\n",
      " -0.16244    -0.31716001 -0.38573     0.03942    -0.26468     1.22529995\n",
      "  0.19279     0.005312   -0.12395    -0.30101001 -0.17156    -0.42899001\n",
      "  0.034108    0.43832999  0.18667001 -0.73627001  0.25948     0.031607\n",
      " -0.39974001 -0.16317999  0.17473     0.33381999  0.24716    -0.57972997\n",
      " -0.020651   -0.041078   -0.49728999 -0.10925    -0.43551001 -0.021357\n",
      " -0.13062    -0.21269999  0.35229999 -0.25628999 -0.62704998  0.073671\n",
      "  0.26864001 -0.45034   ] ...\n",
      "\n",
      "Spacy Token:  .\n",
      "Vector Size:  300\n",
      "[ 0.012001    0.20750999 -0.12578    -0.59324998  0.12525     0.15975\n",
      "  0.13748001 -0.33157    -0.13694     1.78929996 -0.47093999  0.70433998\n",
      "  0.26673001 -0.089961   -0.18167999  0.067226    0.053347    1.55949998\n",
      " -0.25409999  0.038413   -0.01409     0.056774    0.023434    0.024042\n",
      "  0.31703001  0.19024999 -0.37505001  0.035603    0.1181      0.012032\n",
      " -0.037566   -0.50459999 -0.049261    0.092351    0.11031    -0.073062\n",
      "  0.33994001  0.28239     0.13413     0.070128   -0.022099   -0.28103\n",
      "  0.49607    -0.48693001 -0.090964   -0.1538     -0.38011    -0.014228\n",
      " -0.19392    -0.11068   ] ...\n"
     ]
    }
   ],
   "source": [
    "#In spaCy there is functionality to create the word2vec model at the word\n",
    "    #\"The default English model installs vectors for one million vocabulary entries, \n",
    "    #using the 300-dimensional vectors trained on the Common Crawl corpus using the GloVe algorithm. \n",
    "    #The GloVe common crawl vectors have become a de facto standard for practical NLP.\"\n",
    "#https://spacy.io/docs/usage/word-vectors-similarities\n",
    "\n",
    "print('Spacy Token: ', docs[1])\n",
    "print('Vector Size: ', len(cute_vector))\n",
    "cute_vector = docs[1].vector\n",
    "print(cute_vector[0:50], '...')\n",
    "\n",
    "print()\n",
    "\n",
    "print('Spacy Token: ', docs[20])\n",
    "print('Vector Size: ', len(cute_vector))\n",
    "period_vector = docs[20].vector\n",
    "print(period_vector[0:50], '...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The loud furry dog ran and bit at air."
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4_spcy = docs[33:43]\n",
    "s4_spcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cute kitten purred and watched the Stark girl.\n",
      "Vector Length of  300\n",
      "[-0.0394899  -0.0187946  -0.2313281  -0.07501449 -0.0034923  -0.03543136\n",
      " -0.08365459 -0.1366701   0.07988711  1.59997201 -0.22350159 -0.01495151\n",
      "  0.01841    -0.1657521  -0.14396301  0.07315359 -0.0264913   0.67474192\n",
      " -0.152256   -0.06521721 -0.04318    -0.16186132 -0.12671219 -0.10123821\n",
      "  0.0410289   0.0970646  -0.112487   -0.14183018  0.01335313 -0.04647293\n",
      " -0.14982264  0.0205042  -0.0090121   0.168327    0.182576   -0.17904079\n",
      "  0.0069503  -0.05956352 -0.15230086 -0.0496251   0.01015183  0.0113903\n",
      "  0.00332989 -0.16249254  0.0382682  -0.05392995 -0.27333999  0.06541368\n",
      "  0.0911456  -0.227585  ] ...\n",
      "\n",
      "After losing interest the cute furry cat purred and meowed.\n",
      "Vector Length of  300\n",
      "[-0.07320991  0.11364457 -0.18259937 -0.04985264 -0.018801   -0.03465935\n",
      " -0.037993   -0.12700583  0.02468536  1.65511811 -0.21201697  0.05384245\n",
      " -0.07783545 -0.07012409 -0.05808636 -0.06650855 -0.05374527  0.72631359\n",
      " -0.14617108 -0.06922891 -0.07182118 -0.06767427 -0.11332254 -0.116397\n",
      "  0.10679228  0.05630463 -0.08012091 -0.10325181  0.01807976  0.022002\n",
      " -0.15857078 -0.017925    0.01034355  0.08475245  0.14046091 -0.08843163\n",
      "  0.05284046 -0.07485914 -0.0205258  -0.04394236  0.02954002  0.04570173\n",
      "  0.07407293 -0.05270151  0.04326145 -0.00346945 -0.28251508  0.0603629\n",
      " -0.01572791 -0.08664572] ...\n"
     ]
    }
   ],
   "source": [
    "s1_spcy = docs[0:10]\n",
    "s1_vect = s1_spcy.vector\n",
    "s2_spcy = docs[10:21]\n",
    "s2_vect = s2_spcy.vector\n",
    "s3_spcy = docs[21:33]\n",
    "s3_vect = s3_spcy.vector\n",
    "s4_spcy = docs[33:43]\n",
    "s4_vect = s4_spcy.vector\n",
    "\n",
    "print(s1_spcy)\n",
    "print('Vector Length of ', len(sen1_vector))\n",
    "print(sen1_vector[0:50], '...')\n",
    "\n",
    "print()\n",
    "\n",
    "print(s2_spcy)\n",
    "print('Vector Length of ', len(sen2_vector))\n",
    "print(sen2_vector[0:50], '...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cute kitten purred and watched the Stark girl. After losing interest the cute furry cat purred and meowed. Sly yet silent, the cute kitten meowed and she noticed. The loud furry dog ran and bit at air.\n",
      "[ -2.96052452e-02   1.07662223e-01  -1.49132133e-01  -7.46969581e-02\n",
      "   1.32708149e-02   3.17049772e-02  -1.09451404e-02  -1.61642089e-01\n",
      "   2.65648309e-02   1.74241495e+00  -1.46555364e-01   5.43590356e-03\n",
      "  -2.56856922e-02  -1.58875853e-01  -1.62996352e-01   2.48137470e-02\n",
      "  -3.87500040e-02   7.81989813e-01  -2.00948134e-01  -7.90675804e-02\n",
      "  -5.39194196e-02  -6.40327632e-02  -5.70502989e-02  -1.32878065e-01\n",
      "   5.21605499e-02   5.05217910e-02  -1.22933485e-01  -1.20797276e-01\n",
      "   7.05222115e-02  -8.14056993e-02  -1.35169148e-01   1.08062522e-03\n",
      "  -5.00328802e-02   1.31003201e-01   1.24342784e-01  -1.46289453e-01\n",
      "   1.29621802e-03  -3.99797671e-02  -1.01229399e-01  -3.95646989e-02\n",
      "   5.10285832e-02   2.97673959e-02   1.50560727e-02  -1.08856298e-01\n",
      "   9.05850902e-02  -1.38813816e-02  -2.71683395e-01  -2.40068045e-02\n",
      "   1.05061131e-02  -1.12254865e-01]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#Same can be done at the document level\n",
    "print(docs)\n",
    "docs_vector = docs.vector\n",
    "print(docs_vector[0:50])\n",
    "print(len(docs_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim.models.word2vec as w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Rank - find most relevant documents relating to a topic\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 2\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1\n",
    "\n",
    "# select 1 for Skip-Gram Model, 0 for Continuous Bag of Words\n",
    "\n",
    "sg = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initilize Model\n",
    "doc2v = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-9c3117d1f49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;31m# add info about each word's Huffman encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msort_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "doc2v.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 24\n"
     ]
    }
   ],
   "source": [
    "token_count = len(doc2v.wv.vocab)\n",
    "print(\"Word2Vec vocabulary length:\", len(doc2v.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2v.train(sentences, total_examples = token_count, epochs = doc2v.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.52896168e-04,   1.10605743e-03,  -1.51454855e-03,\n",
       "        -1.57380666e-04,  -6.06522080e-04,  -6.79342658e-04,\n",
       "         1.68168708e-03,  -1.05058437e-03,  -4.99991351e-04,\n",
       "        -1.26442651e-03,  -1.08079088e-03,   1.12917065e-03,\n",
       "         5.85765301e-05,  -1.93516654e-03,  -1.48352329e-03,\n",
       "        -4.94567386e-04,   5.51221747e-05,  -7.83927622e-04,\n",
       "        -1.09044416e-03,   1.43233268e-03,   1.22664811e-03,\n",
       "         1.83905271e-04,  -1.56069372e-03,  -1.30228244e-03,\n",
       "         4.17706207e-04,   2.78462161e-04,   7.54722394e-04,\n",
       "        -1.28461979e-03,   1.32872094e-03,  -1.20788650e-03,\n",
       "        -6.61480590e-04,  -1.26445142e-03,   1.69577776e-03,\n",
       "         1.26981537e-03,   3.57229641e-04,  -1.30495685e-03,\n",
       "        -4.09013533e-04,   1.23142847e-03,  -1.15701626e-03,\n",
       "        -1.69484731e-04,   3.67263099e-04,   1.52529997e-03,\n",
       "         1.25181305e-05,   1.46791659e-04,   9.38934332e-04,\n",
       "         1.48172630e-03,  -1.07844837e-03,  -3.50329618e-04,\n",
       "         7.66346697e-04,  -1.52782304e-03,   1.60792362e-04,\n",
       "        -2.72541278e-04,   1.02852471e-03,   2.23423267e-04,\n",
       "        -1.90189050e-03,   1.85129713e-04,   7.39844749e-04,\n",
       "         9.26820969e-04,   2.40535577e-04,   1.42731320e-03,\n",
       "         1.23910047e-03,  -1.51949911e-03,   1.10579794e-03,\n",
       "         1.32290006e-03,  -7.43086392e-04,   9.77293588e-04,\n",
       "         1.49288971e-03,   6.75500080e-04,   6.46219531e-04,\n",
       "        -2.49777135e-04,  -1.23797799e-03,  -1.51888910e-03,\n",
       "        -1.11391512e-03,  -1.06814819e-04,   1.80320101e-04,\n",
       "         7.55664660e-04,   1.60007016e-03,   8.63420777e-04,\n",
       "        -4.21334669e-04,  -1.42997759e-03,  -8.34849605e-04,\n",
       "        -9.45340726e-04,   2.96142476e-04,  -2.10558902e-03,\n",
       "        -6.94815652e-04,  -8.96636571e-04,   1.38194766e-03,\n",
       "        -1.30855176e-03,   7.51948683e-04,   3.73233401e-04,\n",
       "         1.08184693e-04,   1.52892462e-04,   5.39132743e-05,\n",
       "         9.14739561e-04,  -1.55647215e-03,   2.71958474e-04,\n",
       "        -1.45758141e-03,   8.39596556e-04,  -1.00967893e-03,\n",
       "        -1.45767280e-03,   1.02912961e-03,  -1.54205784e-03,\n",
       "         9.21732499e-06,  -2.94113474e-04,   1.40709837e-03,\n",
       "        -3.71931528e-04,   6.59170910e-04,   9.61310347e-04,\n",
       "         8.65539536e-04,  -5.05582066e-05,   1.00417552e-03,\n",
       "        -1.22734590e-03,  -1.13646820e-05,   1.20022707e-03,\n",
       "         2.01845402e-03,   1.65616817e-04,   4.49124898e-04,\n",
       "         1.93436310e-04,  -7.29851366e-04,  -1.11335563e-03,\n",
       "        -1.26754597e-03,   5.70072734e-04,   1.97758360e-04,\n",
       "         1.81788215e-04,   1.62183744e-04,   6.74506533e-04,\n",
       "         1.68880227e-03,  -1.16295356e-03,  -4.37468319e-04,\n",
       "         3.46814137e-04,   1.61986973e-03,  -1.64910933e-04,\n",
       "         1.49652490e-03,  -7.18270254e-04,   1.32183940e-03,\n",
       "         1.28567597e-04,   1.32594735e-03,   5.69775701e-04,\n",
       "        -5.37664397e-04,  -8.30220699e-04,  -3.96005285e-04,\n",
       "        -9.68359062e-04,   5.54662081e-04,   8.51095974e-05,\n",
       "        -1.00915739e-03,  -5.42048729e-05,  -8.91783857e-04,\n",
       "         1.06068495e-04,   1.05527753e-04,  -1.08546822e-03,\n",
       "        -2.53470964e-04,   8.16927641e-05,   1.27313437e-03,\n",
       "        -7.81611016e-04,  -6.73276838e-04,   6.72326947e-04,\n",
       "         1.14417647e-03,   9.71381669e-04,   1.03593722e-03,\n",
       "        -5.15273074e-04,   8.01609771e-04,   1.50888646e-03,\n",
       "        -1.17452105e-03,  -5.59117238e-04,   1.19809783e-03,\n",
       "         1.04589434e-03,   1.98369310e-03,  -1.08135608e-03,\n",
       "        -9.45407315e-04,   1.42473402e-03,  -1.07075740e-03,\n",
       "         1.32280262e-03,  -1.91415264e-03,  -1.40279062e-05,\n",
       "        -1.07710762e-03,   4.19366581e-04,   8.21280410e-04,\n",
       "         9.66842461e-04,   1.21904828e-04,  -5.25498763e-04,\n",
       "         1.99647783e-03,  -5.05001983e-04,   1.50996423e-03,\n",
       "        -1.24355371e-04,   8.43005662e-04,  -1.46165403e-04,\n",
       "        -1.04989402e-03,  -4.61956050e-04,   4.76901594e-04,\n",
       "         1.10812217e-03,   1.03838358e-03,   1.24788389e-03,\n",
       "        -1.24857621e-03,  -4.77841008e-04,  -9.01025196e-04,\n",
       "        -1.20987231e-03,  -1.56877737e-04,  -3.77221702e-04,\n",
       "         1.45754777e-03,  -1.30645314e-03,   3.05796129e-06,\n",
       "        -1.16434041e-03,  -1.21022866e-03,   1.31611421e-03,\n",
       "        -9.12909629e-04,  -1.67617132e-03,   1.86007773e-03,\n",
       "         3.39365477e-04,   1.63367298e-03,  -1.31215237e-03,\n",
       "         1.02169160e-03,   9.92741552e-04,   5.21475333e-04,\n",
       "        -1.42095465e-04,   7.82275922e-04,   6.89308858e-04,\n",
       "         2.76737148e-04,  -1.43295724e-03,  -9.67416679e-04,\n",
       "        -1.17137551e-03,  -5.21209149e-04,  -9.05846013e-04,\n",
       "        -3.55123397e-04,   3.93404334e-04,   1.12069224e-03,\n",
       "        -6.63060637e-05,  -1.46613427e-04,  -1.41741545e-03,\n",
       "         6.39730482e-04,  -5.97739476e-04,   1.01777934e-03,\n",
       "        -4.32956382e-04,   1.87854836e-04,   1.22478756e-03,\n",
       "        -1.35097175e-03,   1.77099241e-03,  -1.19135075e-03,\n",
       "         1.09792349e-03,   1.04654511e-03,  -1.26578740e-03,\n",
       "        -8.48409836e-04,   4.06529027e-04,   3.70829715e-04,\n",
       "         1.59255241e-03,   1.72657869e-03,   1.56779919e-04,\n",
       "         7.19186908e-04,   6.25571294e-04,  -7.99192407e-04,\n",
       "         5.03211690e-04,  -5.03900752e-04,   9.63802857e-04,\n",
       "        -8.14117433e-04,   5.21943322e-04,  -1.49761373e-03,\n",
       "        -1.32870127e-03,  -4.20511380e-04,  -8.01159185e-04,\n",
       "         1.10795384e-03,  -2.50406920e-05,  -1.03038394e-04,\n",
       "         6.87710592e-04,   1.00429123e-03,   8.02333991e-04,\n",
       "        -1.16824184e-03,   4.40529286e-04,  -5.22475340e-04,\n",
       "         1.42817138e-04,  -1.67585351e-03,  -4.58597322e-04,\n",
       "        -1.34228088e-03,  -2.68018193e-04,  -9.69428162e-04,\n",
       "        -3.85727239e-04,  -5.08215919e-04,  -1.77239557e-03,\n",
       "        -3.06114744e-05,  -9.47980967e-04,   1.47256337e-03,\n",
       "         8.95491030e-05,   4.53294429e-04,   2.27991215e-04,\n",
       "        -1.85609824e-04,  -1.78636285e-03,   1.10157870e-03,\n",
       "        -5.37851884e-04,  -1.21883256e-03,  -9.34099779e-04,\n",
       "         2.04996948e-04,  -9.49630165e-04,  -1.17012335e-03,\n",
       "        -1.85575429e-03,   5.02258481e-05,   8.10259604e-04,\n",
       "         4.51546250e-04,   8.69206386e-04,   8.15717911e-04,\n",
       "        -1.35745399e-03,  -5.43047616e-04,  -1.61883805e-03], dtype=float32)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector weight matrix\n",
    "all_word_vectors_matrix = doc2v.wv.syn0\n",
    "print(all_word_vectors_matrix.shape)\n",
    "all_word_vectors_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, cute, kitten, purred, and]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_raw = []\n",
    "for token in docs:\n",
    "    #print(token)\n",
    "    texts_raw.append(token)\n",
    "texts_raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Push Sentences through it\n",
    "w2v2.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `build_vocab` not found.\n"
     ]
    }
   ],
   "source": [
    "?build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'kitten', 'purr', 'watch', 'stark', 'girl', 'lose', 'interest', 'cute', 'furry', 'cat', 'purr', 'meow', 'sly', 'silent', 'cute', 'kitten', 'meow', 'notice', 'loud', 'furry', 'dog', 'run', 'bit', 'air']\n"
     ]
    }
   ],
   "source": [
    "print(texts_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-7b779b65fb5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \"\"\"\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "wv1=w2v2[s1]\n",
    "#tf2=tfidf[s2]\n",
    "#tf3=tfidf[s3]\n",
    "#tf4=tfidf[s4]\n",
    "\n",
    "#tf1N=tfidfN[s1]\n",
    "#tf2N=tfidfN[s2]\n",
    "#tf3N=tfidfN[s3]\n",
    "#tf4N=tfidfN[s4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['Sentence'] = [raw1, raw2, raw3, raw4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tokenization Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['Tokens'] = [texts[0], texts[1], texts[2],texts[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Corpus Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['Corpus'] = [s1, s2, s3, s4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf['TFIDF'] = [tf1, tf2, tf3, tf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['TFIDF Not Normalized'] = [tf1N, tf2N, tf3N, tf4N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sen2Vec Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['Sen2Vec'] = [s1_vect, s2_vect, s3_vect, s4_vect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Corpus</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>TFIDF Not Normalized</th>\n",
       "      <th>Sen2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cute kitten purred and watched the Stark g...</td>\n",
       "      <td>[cute, kitten, purr, watch, stark, girl]</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]</td>\n",
       "      <td>[(0, 0.11024726933725056), (1, 0.2656320682560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (1, 1.0), (2, 1.0), ...</td>\n",
       "      <td>[-0.0394899, -0.0187946, -0.231328, -0.0750145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After losing interest the cute furry cat purr...</td>\n",
       "      <td>[lose, interest, cute, furry, cat, purr, meow]</td>\n",
       "      <td>[(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1...</td>\n",
       "      <td>[(0, 0.10655215922886847), (2, 0.2567289929560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (2, 1.0), (6, 2.0), ...</td>\n",
       "      <td>[-0.0732099, 0.113645, -0.182599, -0.0498526, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sly yet silent, the cute kitten meowed and sh...</td>\n",
       "      <td>[sly, silent, cute, kitten, meow, notice]</td>\n",
       "      <td>[(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (1...</td>\n",
       "      <td>[(0, 0.11024726933725056), (1, 0.2656320682560...</td>\n",
       "      <td>[(0, 0.4150374992788437), (1, 1.0), (10, 1.0),...</td>\n",
       "      <td>[0.00436454, 0.100187, -0.162217, -0.0844011, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The loud furry dog ran and bit at air.</td>\n",
       "      <td>[loud, furry, dog, run, bit, air]</td>\n",
       "      <td>[(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (...</td>\n",
       "      <td>[(8, 0.2182178902359924), (14, 0.4364357804719...</td>\n",
       "      <td>[(8, 1.0), (14, 2.0), (15, 2.0), (16, 2.0), (1...</td>\n",
       "      <td>[-0.0125192, 0.236509, -0.0144205, -0.0900632,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  The cute kitten purred and watched the Stark g...   \n",
       "1   After losing interest the cute furry cat purr...   \n",
       "2   Sly yet silent, the cute kitten meowed and sh...   \n",
       "3             The loud furry dog ran and bit at air.   \n",
       "\n",
       "                                           Tokens  \\\n",
       "0        [cute, kitten, purr, watch, stark, girl]   \n",
       "1  [lose, interest, cute, furry, cat, purr, meow]   \n",
       "2       [sly, silent, cute, kitten, meow, notice]   \n",
       "3               [loud, furry, dog, run, bit, air]   \n",
       "\n",
       "                                              Corpus  \\\n",
       "0   [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]   \n",
       "1  [(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1...   \n",
       "2  [(0, 1), (1, 1), (10, 1), (11, 1), (12, 1), (1...   \n",
       "3  [(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (...   \n",
       "\n",
       "                                               TFIDF  \\\n",
       "0  [(0, 0.11024726933725056), (1, 0.2656320682560...   \n",
       "1  [(0, 0.10655215922886847), (2, 0.2567289929560...   \n",
       "2  [(0, 0.11024726933725056), (1, 0.2656320682560...   \n",
       "3  [(8, 0.2182178902359924), (14, 0.4364357804719...   \n",
       "\n",
       "                                TFIDF Not Normalized  \\\n",
       "0  [(0, 0.4150374992788437), (1, 1.0), (2, 1.0), ...   \n",
       "1  [(0, 0.4150374992788437), (2, 1.0), (6, 2.0), ...   \n",
       "2  [(0, 0.4150374992788437), (1, 1.0), (10, 1.0),...   \n",
       "3  [(8, 1.0), (14, 2.0), (15, 2.0), (16, 2.0), (1...   \n",
       "\n",
       "                                             Sen2Vec  \n",
       "0  [-0.0394899, -0.0187946, -0.231328, -0.0750145...  \n",
       "1  [-0.0732099, 0.113645, -0.182599, -0.0498526, ...  \n",
       "2  [0.00436454, 0.100187, -0.162217, -0.0844011, ...  \n",
       "3  [-0.0125192, 0.236509, -0.0144205, -0.0900632,...  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "#From https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "    #weight_{i,j} = frequency_{i,j} * log_2(D / document_freq_{i})\n",
    "    = \n",
    "\n",
    "# 'loud' in row 3 TF = 1; D = 4; DF = 1  -> TF/DF = 1 * log_2(4/1)\n",
    "# 'cute' in row 0 sentance TF = 1 & DF = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
